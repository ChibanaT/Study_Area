{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0360294",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network\n",
    "\n",
    "Feedforward refers to recognition-inference architecture of neural networks. Artificial neural network architectures are based on inputs multiplied by weights to obtain outputs (inputs-to-output): feedforward.Recurrent neural networks, or neural networks with loops allow information from later processing stages to feed back to earlier stages for sequence processing. However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation or backpropagation through time. Thus neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, because this forms an infinite loop which is not possible to rewind in time to generate an error signal through backpropagation. This issue and nomenclature appear to be a point of confusion between some computer scientists and scientists in other fields studying brain networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbff61",
   "metadata": {},
   "source": [
    "## Simple Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56dddf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe5b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Activation Function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the Derivative of the Activation Function\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224dca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy input and Output Data\n",
    "\n",
    "# Input: 4 samples with 2 features each\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "# Expected output for XOR problem\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights and Biases\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "input_neurons = X.shape[1]         # Number of input features\n",
    "hidden_neurons = 4                  # Number of neurons in the hidden layer\n",
    "output_neurons = 1                 # Number of output neurons\n",
    "\n",
    "# Weights between input and hidden layer\n",
    "W1 = np.random.rand(input_neurons, hidden_neurons)\n",
    "# Bias for hidden layer\n",
    "B1 = np.random.rand(1, hidden_neurons)\n",
    "\n",
    "# Weights between hidden and output layer\n",
    "W2 = np.random.rand(hidden_neurons, output_neurons)\n",
    "# Bias for output layer\n",
    "B2 = np.random.rand(1, output_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c344325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3322\n",
      "Epoch 1000, Loss: 0.2490\n",
      "Epoch 2000, Loss: 0.2444\n",
      "Epoch 3000, Loss: 0.2180\n",
      "Epoch 4000, Loss: 0.1561\n",
      "Epoch 5000, Loss: 0.0560\n",
      "Epoch 6000, Loss: 0.0209\n",
      "Epoch 7000, Loss: 0.0109\n",
      "Epoch 8000, Loss: 0.0069\n",
      "Epoch 9000, Loss: 0.0049\n"
     ]
    }
   ],
   "source": [
    "# Training Loop (Forward and Backward Propagation)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---Forward Propagation---\n",
    "    Z1 = np.dot(X, W1) + B1  # Weighted sum for hidden layer\n",
    "    A1 = sigmoid(Z1)         # Activation for hidden layer\n",
    "\n",
    "    Z2 = np.dot(A1, W2) + B2  # Weighted sum for output layer\n",
    "    A2 = sigmoid(Z2)          # Activation for output layer\n",
    "\n",
    "    # ---Backward Propagation---\n",
    "    error = y - A2  # Calculate error\n",
    "    dA2 = error * sigmoid_derivative(A2)  # Derivative of output layer\n",
    "\n",
    "    error_hidden = dA2.dot(W2.T)  # Backpropagation to hidden layer\n",
    "    dA1 = error_hidden * sigmoid_derivative(A1)  # Derivative of hidden layer\n",
    "\n",
    "    # ---Update Weights and Biases---\n",
    "    W2 += A1.T.dot(dA2) * learning_rate  # Update weights for output layer\n",
    "    B2 += np.sum(dA2, axis=0, keepdims=True) * learning_rate  # Update bias for output layer\n",
    "\n",
    "    W1 += X.T.dot(dA1) * learning_rate  # Update weights for hidden layer\n",
    "    B1 += np.sum(dA1, axis=0, keepdims=True) * learning_rate  # Update bias for hidden layer\n",
    "\n",
    "    # ---Print loss occasionally---\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))  # Mean Squared Error\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ffe4ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output after Training:\n",
      "[[0.05]\n",
      " [0.96]\n",
      " [0.93]\n",
      " [0.07]]\n"
     ]
    }
   ],
   "source": [
    "# Test the Output\n",
    "\n",
    "print(\"Final Output after Training:\")\n",
    "print(A2.round(2))  # Output after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213a34f",
   "metadata": {},
   "source": [
    "## PyTorch Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d189622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf2b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: 4 samples with 2 features\n",
    "X = torch.tensor([[0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1]], dtype = torch.float32)\n",
    "\n",
    "# Target output\n",
    "y = torch.tensor([[0],\n",
    "                [1],\n",
    "                [1],\n",
    "                [0]], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3107eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)      # 2 input features, 4 hidden neurons\n",
    "        self.fc2 = nn.Linear(4, 1)      # 4 hidden neurons, 1 output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))  # Hidden layer activation\n",
    "        x = torch.sigmoid(self.fc2(x))  # Output layer activation\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a8baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model, Loss Function and Optimizer\n",
    "\n",
    "model = FFNN()\n",
    "\n",
    "# Binary Cross Entropy is suitable for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Stochastic Gradient Descent optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89924a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tchibana\\AppData\\Local\\Temp\\ipykernel_635732\\3067645477.py:18: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7006\n",
      "Epoch 1000, Loss: 0.6920\n",
      "Epoch 2000, Loss: 0.6867\n",
      "Epoch 3000, Loss: 0.6283\n",
      "Epoch 4000, Loss: 0.3510\n",
      "Epoch 5000, Loss: 0.1201\n",
      "Epoch 6000, Loss: 0.0594\n",
      "Epoch 7000, Loss: 0.0375\n",
      "Epoch 8000, Loss: 0.0268\n",
      "Epoch 9000, Loss: 0.0206\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---Forward Pass---\n",
    "    outputs = model(X)  # Forward pass\n",
    "    loss = criterion(outputs, y)  # Calculate loss\n",
    "\n",
    "    # ---Backward Pass---\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    # ---Print loss occasionally---\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9d1a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "\n",
    "with torch.no_grad(): # no gradient needed\n",
    "    predictions = model(X)  # Forward pass\n",
    "    print(\"Predicted Output:\")\n",
    "    print(predictions.round())  # Round the output to get binary predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db224f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Check if CUDA is available\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35373bf",
   "metadata": {},
   "source": [
    "## Pytorch With GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fd5f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a96f4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Select the device: GPU if available, else CPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Print the selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3621851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: 4 samples with 2 features\n",
    "X = torch.tensor([[0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1]], dtype = torch.float32).to(device) # Move to device\n",
    "\n",
    "# Target output\n",
    "y = torch.tensor([[0],\n",
    "                [1],\n",
    "                [1],\n",
    "                [0]], dtype = torch.float32).to(device) # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e848ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)      # 2 input features, 4 hidden neurons\n",
    "        self.fc2 = nn.Linear(4, 1)      # 4 hidden neurons, 1 output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))  # Hidden layer activation\n",
    "        x = torch.sigmoid(self.fc2(x))  # Output layer activation\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "158ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model, Loss Function and Optimizer\n",
    "\n",
    "model = FFNN().to(device)  # Move model to device\n",
    "\n",
    "# Binary Cross Entropy is suitable for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Stochastic Gradient Descent optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "677e7430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7079\n",
      "Epoch 1000, Loss: 0.6932\n",
      "Epoch 2000, Loss: 0.6930\n",
      "Epoch 3000, Loss: 0.6928\n",
      "Epoch 4000, Loss: 0.6918\n",
      "Epoch 5000, Loss: 0.6853\n",
      "Epoch 6000, Loss: 0.6293\n",
      "Epoch 7000, Loss: 0.4138\n",
      "Epoch 8000, Loss: 0.1478\n",
      "Epoch 9000, Loss: 0.0680\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---Forward Pass---\n",
    "    outputs = model(X)  # Forward pass\n",
    "    loss = criterion(outputs, y)  # Calculate loss\n",
    "\n",
    "    # ---Backward Pass---\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    # ---Print loss occasionally---\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f2b1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "\n",
    "with torch.no_grad(): # no gradient needed\n",
    "    predictions = model(X)  # Forward pass\n",
    "    print(\"Predicted Output:\")\n",
    "    print(predictions.round().cpu())  # Round the output to get binary predictions and move to CPU for printing\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
